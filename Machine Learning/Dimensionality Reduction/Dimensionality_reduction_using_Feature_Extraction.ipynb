{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dimensionality reduction using Feature Extraction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPMNUrjxlX0nuQRhR6Vs77W"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dimensionality reduction using Feature Extraction"
      ],
      "metadata": {
        "id": "4wxuwDos2dfc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Reducing Features Using Principal Components**\n",
        "**Problem**\n",
        "\n",
        "\n",
        "Given a set of features, you want to reduce the number of features while retaining the variance in the data.\n",
        "\n",
        "\n",
        "**Solution**\n",
        "\n",
        "\n",
        "Use principal component analysis with scikit’s PCA:\n",
        "\n"
      ],
      "metadata": {
        "id": "wDYrDwG4cxzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manan Hakani - Data Science\n",
        "\n",
        "# Load libraries\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import datasets\n",
        "\n",
        "# Load the data\n",
        "digits = datasets.load_digits()\n",
        "\n",
        "# Standardize the feature matrix\n",
        "X = StandardScaler().fit_transform(digits.data)\n",
        "\n",
        "# Create a PCA that will retain 99% of the variance\n",
        "pca = PCA(n_components=0.99, whiten=True)\n",
        "\n",
        "# Conduct PCA\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Show results\n",
        "print('Original number of features:', X.shape[1])\n",
        "print('Reduced number of features:', X_pca.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQlrTueibV9o",
        "outputId": "676d30e5-7c17-4879-de23-8b21dda062c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of features: 64\n",
            "Reduced number of features: 54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reducing Features When Data Is Linearly Inseparable**\n",
        "**Problem**\n",
        "\n",
        "You suspect you have linearly inseparable data and want to reduce the dimensions.\n",
        "\n",
        "**Solution**\n",
        "\n",
        "Use an extension of principal component analysis that uses `kernels` to allow for non-linear dimensionality reduction:"
      ],
      "metadata": {
        "id": "fbA5RwGmc85g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manan Hakani - Data Science\n",
        "\n",
        "# Load libraries\n",
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "from sklearn.datasets import make_circles\n",
        "\n",
        "# Create linearly inseparable data\n",
        "X, _ = make_circles(n_samples=1000, random_state=1, noise=0.1, factor=0.1)\n",
        "\n",
        "# Apply kernal PCA with radius basis function (RBF) kernel\n",
        "kpca = KernelPCA(kernel=\"rbf\", gamma=15, n_components=1)\n",
        "X_kpca = kpca.fit_transform(X)\n",
        "\n",
        "print('Original number of features:', X.shape[1])\n",
        "print('Reduced number of features:', X_kpca.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSkp4xSTdCzn",
        "outputId": "f4271d16-055a-4678-f070-df586fa9ad55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of features: 2\n",
            "Reduced number of features: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Kernel PCA\n",
        "\n",
        "\n",
        "In our solution \n",
        "\n",
        "we used scikit-learn’s `make_circles` to generate a simulated dataset with a target vector of two classes and two features. \n",
        "make_circles makes linearly inseparable data; specifically, one class is surrounded on all sides by the other class."
      ],
      "metadata": {
        "id": "O57YBtsD7PWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Reducing Features by Maximizing Class Separability**\n",
        "**Problem**\n",
        "\n",
        "You want to reduce the features to be used by a classifier.\n",
        "\n",
        "**Solution**\n",
        "\n",
        "Try `linear discriminant analysis (LDA)` to project the features onto component axes that maximize the separation of classes:"
      ],
      "metadata": {
        "id": "2DmJtiV_U6c6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manan Hakani - Data Science\n",
        "\n",
        "# Load libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "# Load the Iris flower dataset:\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Create an LDA that will reduce the data down to 1 feature\n",
        "lda = LinearDiscriminantAnalysis(n_components=1)\n",
        "\n",
        "# run an LDA and use it to transform the features\n",
        "X_lda = lda.fit(X, y).transform(X)\n",
        "\n",
        "# Print the number of features\n",
        "print('Original number of features:', X.shape[1])\n",
        "print('Reduced number of features:', X_lda.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSECTiq4U7xS",
        "outputId": "c087256e-3745-4f00-d70c-e09208745eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of features: 4\n",
            "Reduced number of features: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use `explained_variance_ratio_` to view the amount of variance explained by each component. In our solution the single component explained over 99% of the variance:"
      ],
      "metadata": {
        "id": "S48BNPwqRWYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## View the ratio of explained variance\n",
        "lda.explained_variance_ratio_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRlKFVgsRX4N",
        "outputId": "6fccda2d-13ec-471d-bd1c-ef15f0589676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.9912126])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}